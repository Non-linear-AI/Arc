## TASK
{% if existing_yaml and editing_instructions %}Edit an existing data processing JSON configuration based on the following changes:
- **Editing Instructions**: {{ editing_instructions }}
- **Original Requirements**: {{ user_context }}
{% if target_tables %}- **Target Tables**: {{ target_tables | join(", ") }}
{% endif %}- **Database**: {{ schema_info.database }}
{% else %}Generate a data processing JSON configuration for:
- **User Requirements**: {{ user_context }}
{% if target_tables %}- **Target Tables**: {{ target_tables | join(", ") }}
{% endif %}- **Database**: {{ schema_info.database }}
{% endif %}
{% if ml_plan_feature_engineering %}

## ML PLAN FEATURE ENGINEERING GUIDANCE

The following feature engineering strategy was recommended by the ML Plan tool after analyzing the data characteristics and ML task requirements:

{{ ml_plan_feature_engineering }}

**CRITICAL**: This guidance from the ML Plan represents a carefully considered feature engineering design. Follow this strategy closely when generating the data processing pipeline to ensure the features are properly prepared for the ML model.
{% endif %}

## Available Tables
{% if schema_info.tables %}{% for table in schema_info.tables %}**{{ table.name }}**:
{% for column in table.columns %}  - {{ column.name }} ({{ column.type }}{% if not column.nullable %}, NOT NULL{% endif %})
{% endfor %}

{% endfor %}{% else %}No table schema information available.
{% endif %}
{% if existing_yaml and editing_instructions %}
## Existing Configuration
The current YAML configuration that needs to be edited:
```yaml
{{ existing_yaml }}
```

Apply the editing instructions above to this configuration. Make the requested changes while preserving the overall structure and any parts not affected by the edits.

{% endif %}
## JSON Schema
You must generate JSON that follows this exact schema:

```json
{
  "type": "object",
  "required": ["name", "description", "steps", "outputs"],
  "properties": {
    "name": {"type": "string", "description": "Name of the data processing pipeline"},
    "description": {"type": "string", "description": "Description of what this pipeline does"},
    "vars": {
      "type": "object",
      "description": "Optional variables for SQL substitution (${var} syntax)"
    },
    "steps": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["name", "depends_on", "sql"],
        "properties": {
          "name": {"type": "string", "description": "Unique name for this processing step"},
          "depends_on": {
            "type": "array",
            "items": {"type": "string"},
            "description": "List of table names or step names this step depends on"
          },
          "sql": {"type": "string", "description": "Concrete SQL query for this transformation step"}
        }
      }
    },
    "outputs": {
      "type": "array",
      "items": {"type": "string"},
      "description": "List of step names that should be materialized as final output tables"
    }
  }
}
```

## Example JSON Structure
```json
{
  "name": "user_transaction_aggregates",
  "description": "Process transaction data to create user-level aggregates",
  "steps": [
    {
      "name": "clean_data",
      "depends_on": ["source_table"],
      "sql": "SELECT * FROM source_table WHERE date >= '2023-01-01' AND amount > 100"
    },
    {
      "name": "user_aggregates",
      "depends_on": ["clean_data"],
      "sql": "SELECT user_id, COUNT(*) as transaction_count, SUM(amount) as total_amount FROM clean_data GROUP BY user_id"
    }
  ],
  "outputs": ["user_aggregates"]
}
```

## Rules
1. **Step Names**: Use descriptive names (clean_data, user_aggregates, final_features)
2. **Dependencies**: Each step must list all tables/steps it depends on in the depends_on array
3. **SQL Quality**: Write clean, efficient SQL with concrete values
4. **Concrete SQL**: Write specific SQL queries with actual values, dates, and thresholds - no variables or placeholders
5. **Outputs**: Only list steps that should be persisted as tables in the outputs array
6. **Execution Order**: Steps will be executed in dependency order automatically
7. **JSON Format**: Must be valid JSON with proper escaping of quotes in SQL strings

## Production-Ready Configuration Requirements
8. **Completeness**: Generate a COMPREHENSIVE configuration that requires NO further modifications
9. **Data Quality**: Include proper data validation, null handling, and edge case management
10. **Error Prevention**: Anticipate common data issues and include appropriate safeguards
11. **Best Practices**: Follow SQL best practices for performance and maintainability
12. **Schema Compliance**: Ensure the configuration strictly follows the DataSourceSpec schema

## Common Patterns
- **Data Cleaning**: Filter nulls, invalid values, standardize formats
- **Aggregations**: GROUP BY with COUNT, SUM, AVG for time periods or categories
- **Feature Engineering**: Create calculated fields, ratios, time-based features
- **Joins**: Combine data from multiple tables
- **Windowing**: Running totals, rankings, time-based calculations


## Output Requirements
- Generate only valid JSON, no markdown blocks or explanations
- Include name and description fields at the root level
- Ensure all strings are properly escaped
- Follow the exact schema structure above
- Create a COMPLETE, PRODUCTION-READY configuration that needs no enhancements

## Critical Instruction
{% if existing_yaml and editing_instructions %}Your edited configuration must maintain the same level of quality and completeness as the original. Apply only the requested changes while preserving the production-ready nature of the configuration.

Edit the data processing configuration according to the instructions:
{% else %}Your generated configuration must be comprehensive and final. It will be used directly in production without any modifications. Include all necessary data processing steps, validations, and transformations in your first response.

Generate the data processing configuration:
{% endif %}