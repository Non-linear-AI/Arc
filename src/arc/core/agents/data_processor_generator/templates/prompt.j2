## TASK
Generate a data processing JSON configuration for:
- **User Requirements**: {{ user_context }}
{% if target_tables %}- **Target Tables**: {{ target_tables | join(", ") }}
{% endif %}- **Database**: {{ schema_info.database }}

## Available Tables
{% if schema_info.tables %}{% for table in schema_info.tables %}**{{ table.name }}**:
{% for column in table.columns %}  - {{ column.name }} ({{ column.type }}{% if not column.nullable %}, NOT NULL{% endif %})
{% endfor %}

{% endfor %}{% else %}No table schema information available.
{% endif %}
## JSON Schema
You must generate JSON that follows this exact schema:

```json
{
  "type": "object",
  "required": ["data_source"],
  "properties": {
    "data_source": {
      "type": "object",
      "required": ["steps", "outputs"],
      "properties": {
        "steps": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["name", "depends_on", "sql"],
            "properties": {
              "name": {"type": "string", "description": "Unique name for this processing step"},
              "depends_on": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of table names or step names this step depends on"
              },
              "sql": {"type": "string", "description": "Concrete SQL query for this transformation step"}
            }
          }
        },
        "outputs": {
          "type": "array",
          "items": {"type": "string"},
          "description": "List of step names that should be materialized as final output tables"
        }
      }
    }
  }
}
```

## Example JSON Structure
```json
{
  "data_source": {
    "steps": [
      {
        "name": "clean_data",
        "depends_on": ["source_table"],
        "sql": "SELECT * FROM source_table WHERE date >= '2023-01-01' AND amount > 100"
      },
      {
        "name": "user_aggregates",
        "depends_on": ["clean_data"],
        "sql": "SELECT user_id, COUNT(*) as transaction_count, SUM(amount) as total_amount FROM clean_data GROUP BY user_id"
      }
    ],
    "outputs": ["user_aggregates"]
  }
}
```

## Rules
1. **Step Names**: Use descriptive names (clean_data, user_aggregates, final_features)
2. **Dependencies**: Each step must list all tables/steps it depends on in the depends_on array
3. **SQL Quality**: Write clean, efficient SQL with concrete values
4. **Concrete SQL**: Write specific SQL queries with actual values, dates, and thresholds - no variables or placeholders
5. **Outputs**: Only list steps that should be persisted as tables in the outputs array
6. **Execution Order**: Steps will be executed in dependency order automatically
7. **JSON Format**: Must be valid JSON with proper escaping of quotes in SQL strings

## Production-Ready Configuration Requirements
8. **Completeness**: Generate a COMPREHENSIVE configuration that requires NO further modifications
9. **Data Quality**: Include proper data validation, null handling, and edge case management
10. **Error Prevention**: Anticipate common data issues and include appropriate safeguards
11. **Best Practices**: Follow SQL best practices for performance and maintainability
12. **Schema Compliance**: Ensure the configuration strictly follows the DataSourceSpec schema

## Common Patterns
- **Data Cleaning**: Filter nulls, invalid values, standardize formats
- **Aggregations**: GROUP BY with COUNT, SUM, AVG for time periods or categories
- **Feature Engineering**: Create calculated fields, ratios, time-based features
- **Joins**: Combine data from multiple tables
- **Windowing**: Running totals, rankings, time-based calculations


## Output Requirements
- Generate only valid JSON, no markdown blocks or explanations
- Start with the root object containing data_source
- Ensure all strings are properly escaped
- Follow the exact schema structure above
- Create a COMPLETE, PRODUCTION-READY configuration that needs no enhancements

## Critical Instruction
Your generated configuration must be comprehensive and final. It will be used directly in production without any modifications. Include all necessary data processing steps, validations, and transformations in your first response.

Generate the data processing configuration: