# Arc Tool Definitions
# This file defines all tools available to the Arc agent.
# Tools are loaded dynamically and converted to ArcTool instances.

tools:
  - name: view_file
    description: View contents of a file or list directory contents
    parameters:
      type: object
      properties:
        path:
          type: string
          description: Path to file or directory to view
        start_line:
          type: integer
          description: Starting line number for partial file view (optional)
        end_line:
          type: integer
          description: Ending line number for partial file view (optional)
      required:
        - path

  - name: create_file
    description: Create a new file with specified content
    parameters:
      type: object
      properties:
        path:
          type: string
          description: Path where the file should be created
        content:
          type: string
          description: Content to write to the file
      required:
        - path
        - content

  - name: edit_file
    description: Edit an existing file by replacing text using EXACT string matching. The old_str must match exactly including whitespace and line breaks. If the string is not found or appears multiple times, the tool will return a clear error. Use view_file first to see the exact text to match.
    parameters:
      type: object
      properties:
        path:
          type: string
          description: Path to the file to edit
        old_str:
          type: string
          description: Text to replace (must match exactly including all whitespace and line breaks)
        new_str:
          type: string
          description: Text to replace with
        replace_all:
          type: boolean
          description: Replace all occurrences (default false, only replaces first occurrence)
      required:
        - path
        - old_str
        - new_str

  - name: bash
    description: Execute a bash command
    parameters:
      type: object
      properties:
        command:
          type: string
          description: The bash command to execute
      required:
        - command

  - name: search
    description: Unified search tool for finding text content or files
    parameters:
      type: object
      properties:
        query:
          type: string
          description: Text to search for or file name/path pattern
        search_type:
          type: string
          enum:
            - text
            - files
            - both
          description: "Type of search: 'text' for content search, 'files' for file names, 'both' for both (default: 'both')"
        include_pattern:
          type: string
          description: "Glob pattern for files to include (e.g. '*.ts', '*.js')"
        exclude_pattern:
          type: string
          description: "Glob pattern for files to exclude (e.g. '*.log', 'node_modules')"
        case_sensitive:
          type: boolean
          description: Whether search should be case sensitive (default false)
        whole_word:
          type: boolean
          description: Whether to match whole words only (default false)
        regex:
          type: boolean
          description: Whether query is a regex pattern (default false)
        max_results:
          type: integer
          description: Maximum number of results to return (default 50)
        file_types:
          type: array
          items:
            type: string
          description: "File types to search (e.g. ['js', 'ts', 'py'])"
        include_hidden:
          type: boolean
          description: Whether to include hidden files (default false)
      required:
        - query

  - name: create_todo_list
    description: Create a new todo list for planning and tracking tasks
    parameters:
      type: object
      properties:
        todos:
          type: array
          description: Array of todo items to create
          items:
            type: object
            properties:
              id:
                type: string
                description: Unique identifier for the todo item
              content:
                type: string
                description: Description of the todo item
              status:
                type: string
                enum:
                  - pending
                  - in_progress
                  - completed
                description: Current status of the todo item (default pending)
            required:
              - id
              - content
              - status
      required:
        - todos

  - name: update_todo_list
    description: Update existing todos in the todo list by ID
    parameters:
      type: object
      properties:
        updates:
          type: array
          description: Array of todo updates
          items:
            type: object
            properties:
              id:
                type: string
                description: ID of the todo item to update
              status:
                type: string
                enum:
                  - pending
                  - in_progress
                  - completed
                description: New status for the todo item
              content:
                type: string
                description: New content for the todo item
              priority:
                type: string
                enum:
                  - high
                  - medium
                  - low
                description: New priority for the todo item
            required:
              - id
      required:
        - updates

  - name: database_query
    description: |
      Execute READ-ONLY SQL queries (SELECT, SHOW, DESCRIBE) for data exploration and analysis.
      Returns results immediately without confirmation. For data transformations that write/modify
      data (INSERT, UPDATE, DELETE, CREATE TABLE, etc.), use ml_data instead.

      DATABASE ENGINE: DuckDB
      - Use DuckDB-compatible SQL syntax
      - For metadata: Use SHOW TABLES, SHOW VIEWS, or information_schema
    parameters:
      type: object
      properties:
        query:
          type: string
          description: SQL query to execute
        target_db:
          type: string
          enum:
            - system
            - user
          description: |
            Target database (default: 'user'):
            - 'system': Arc ML metadata (models, jobs, training_runs, evaluation_runs, ml_plans, data_processors) - read-only
            - 'user': User's training/application data (full SQL access)

            IMPORTANT: Always specify target_db='system' when querying ML metadata tables like:
            - jobs (training jobs, evaluation jobs)
            - models (registered models)
            - training_runs (training metrics and logs)
            - evaluation_runs (evaluation results)
            - ml_plans (ML workflow plans)
            - data_processors (data processing pipelines)
        validate_schema:
          type: boolean
          description: Whether to validate query against database schema (default true)
      required:
        - query

  - name: schema_discovery
    description: Discover and explore database schema information
    parameters:
      type: object
      properties:
        action:
          type: string
          enum:
            - list_tables
            - describe_table
            - show_schema
          description: "Schema discovery action: 'list_tables' shows all tables, 'describe_table' shows detailed table structure, 'show_schema' shows complete database overview"
        target_db:
          type: string
          enum:
            - system
            - user
          description: "Target database: 'system' for Arc metadata, 'user' for training data (default: system)"
        table_name:
          type: string
          description: Specific table name (required for 'describe_table' action)
      required:
        - action

  - name: ml_model
    description: |
      Generate unified model + training specification and launch training. This tool:
      1) Generates model architecture AND training config in a single unified YAML
      2) Presents interactive confirmation workflow
      3) Auto-registers model to database after approval
      4) Creates trainer from training config
      5) Launches training immediately

      Use the knowledge_references parameter to suggest relevant architecture knowledge IDs
      (e.g., ['mlp', 'dcn', 'transformer']) for the model generation sub-agent to explore.

      Returns JSON with the following structure:
      {
        "status": "accepted" | "cancelled",
        "model_id": "model-v1",
        "model_spec": {
          "inputs": {...},
          "graph": {...},
          "outputs": {...},
          "loss": {...},
          "training": {...}
        },
        "training": {
          "status": "submitted" | "failed" | "not_started",
          "job_id": "job_xyz789",
          "train_table": "training_data",
          "error": "..." // only if failed
        }
      }

      The model_spec contains the full architecture and training configuration. Extract job_id
      to monitor training progress with /ml jobs status.
    parameters:
      type: object
      properties:
        name:
          type: string
          description: Experiment name (used for both model and trainer)
        instruction:
          type: string
          description: Description of model architecture and training requirements
        data_table:
          type: string
          description: Database table to profile for generation
        train_table:
          type: string
          description: Training data table (defaults to data_table if not provided)
        target_column:
          type: string
          description: Target column for prediction
        data_processing_id:
          type: string
          description: |
            Optional data processing execution ID from ml_data tool result. When provided,
            loads SQL transformations and output schemas to understand feature engineering.
            Extract this from ml_data's JSON result: result["data_processing_id"]
        knowledge_references:
          type: array
          items:
            type: string
          description: |
            Optional list of knowledge IDs to suggest to the model generation sub-agent
            (e.g., ['mlp', 'dcn']). The sub-agent will use its own knowledge tools to
            access these or discover additional knowledge as needed.
      required:
        - name
        - instruction
        - data_table
        - target_column

  - name: ml_evaluate
    description: |
      Evaluate a trained model on a test dataset and compute metrics.
      This is an AD HOC tool for evaluating trained models on separate test datasets.
      It is NOT part of the core ML workflow (plan → data → model → train). Validation
      metrics are already tracked during training.

      This tool:
      1) Infers target column from the model's specification
      2) Creates evaluator configuration automatically
      3) Auto-registers evaluator to database
      4) Launches evaluation job and returns immediately with job_id

      Evaluation runs asynchronously in the background. Monitor progress via /ml jobs status
      and view metrics in TensorBoard. Supports classification metrics (accuracy, precision,
      recall, F1, confusion matrix, PR curves, ROC curves) and regression metrics (MSE, MAE,
      R², scatter plots).

      Use this tool when you need to:
      - Evaluate a trained model on a hold-out test set
      - Compare multiple models on the same test data
      - Generate comprehensive evaluation reports with visualizations

      Returns JSON with the following structure:
      {
        "status": "accepted" | "cancelled",
        "evaluator_id": "evaluator-id-v1",
        "evaluation": {
          "status": "submitted" | "failed" | "not_started",
          "job_id": "job_abc123",
          "model_id": "model-v1",
          "dataset": "test_data",
          "error": "..." // only if failed
        }
      }

      The evaluation runs asynchronously. Extract job_id to monitor progress with /ml jobs status.
      View detailed metrics and visualizations in TensorBoard.
    parameters:
      type: object
      properties:
        model_id:
          type: string
          description: Model ID with version (e.g., 'my-model-v1')
        data_table:
          type: string
          description: Test dataset table name for evaluation
        metrics:
          type: array
          items:
            type: string
          description: |
            Optional list of specific metrics to compute. If not provided, metrics are
            inferred from the model's task type (classification or regression).
            Available classification metrics: accuracy, precision, recall, f1_score, auc
            Available regression metrics: mse, mae, rmse, r2_score
        output_table:
          type: string
          description: |
            Optional table name to save predictions with features and targets.
            If not provided, predictions are not saved to database.
      required:
        - model_id
        - data_table

  - name: ml_data
    description: |
      Generate and execute SQL data processing pipelines for WRITE OPERATIONS.
      Use when users describe data transformations, feature engineering, or operations
      that create/modify tables (INSERT, UPDATE, DELETE, CREATE TABLE, etc.).
      Workflow:
      1) Generate YAML from description,
      2) Show user for review/editing,
      3) Execute pipeline after confirmation,
      4) Create output tables.
      For read-only SELECT queries, use database_query for immediate results.

      Use the knowledge_references parameter to suggest relevant knowledge IDs
      (e.g., ['ml_data_preparation']) for the data processing sub-agent to explore.

      Returns JSON with the following structure:
      {
        "status": "accepted" | "cancelled",
        "data_processing_id": "data_abc123",
        "execution": {
          "output_tables": ["table1", "table2"],
          "sql_operations": ["CREATE TABLE table1 AS SELECT...", "CREATE TABLE table2 AS SELECT..."]
        }
      }

      The data_processing_id can be passed to ml_model to link the model to specific
      data processing. The sql_operations provide context about the DB transactions performed.
    parameters:
      type: object
      properties:
        name:
          type: string
          description: Name for the data processor (will be registered in database)
        instruction:
          type: string
          description: Natural language description of data processing requirements
        source_tables:
          type: array
          items:
            type: string
          description: List of source database tables to analyze for generation (required to narrow scope)
        database:
          type: string
          enum:
            - system
            - user
          description: "Target database: 'system' for Arc metadata, 'user' for training data (default: user)"
        knowledge_references:
          type: array
          items:
            type: string
          description: |
            Optional list of knowledge IDs to suggest to the data processing sub-agent
            (e.g., ['ml_data_preparation']). The sub-agent will use its own knowledge tools
            to access these or discover additional knowledge as needed.
      required:
        - name
        - instruction
        - source_tables

  - name: list_available_knowledge
    description: |
      List all available knowledge documents including architectures, patterns, and scenarios.
      Returns metadata with knowledge ID, name, description, and applicable phases (model, trainer, evaluator).

      Use this tool to:
      - Explore available architectural patterns (MLP, DCN, Transformer, etc.)
      - Discover training strategies and evaluation techniques
      - Find data processing patterns and best practices
    parameters:
      type: object
      properties: {}

  - name: read_knowledge
    description: |
      Read domain knowledge documents to inform model/trainer/evaluator generation.
      Knowledge includes architectures (dcn, transformer), patterns (feature-interaction,
      regularization), and scenarios (fraud-detection, churn-prediction).

      Use this tool to:
      - Understand architecture details and implementation guidance
      - Learn problem-specific patterns and best practices
      - Discover relevant techniques and patterns

      Use list_available_knowledge first to see what's available, then read specific documents.
    parameters:
      type: object
      properties:
        knowledge_id:
          type: string
          description: "ID of the knowledge to read (e.g., 'dcn', 'feature-interaction')"
        phase:
          type: string
          enum: [model, trainer, evaluator]
          description: "Which phase guide to read (default: model)"
      required:
        - knowledge_id
