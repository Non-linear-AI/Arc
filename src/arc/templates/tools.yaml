# Arc Tool Definitions
# This file defines all tools available to the Arc agent.
# Tools are loaded dynamically and converted to ArcTool instances.

tools:
  - name: view_file
    description: View contents of a file or list directory contents
    parameters:
      type: object
      properties:
        path:
          type: string
          description: Path to file or directory to view
        start_line:
          type: integer
          description: Starting line number for partial file view (optional)
        end_line:
          type: integer
          description: Ending line number for partial file view (optional)
      required:
        - path

  - name: create_file
    description: Create a new file with specified content
    parameters:
      type: object
      properties:
        path:
          type: string
          description: Path where the file should be created
        content:
          type: string
          description: Content to write to the file
      required:
        - path
        - content

  - name: edit_file
    description: Edit an existing file by replacing text using EXACT string matching. The old_str must match exactly including whitespace and line breaks. If the string is not found or appears multiple times, the tool will return a clear error. Use view_file first to see the exact text to match.
    parameters:
      type: object
      properties:
        path:
          type: string
          description: Path to the file to edit
        old_str:
          type: string
          description: Text to replace (must match exactly including all whitespace and line breaks)
        new_str:
          type: string
          description: Text to replace with
        replace_all:
          type: boolean
          description: Replace all occurrences (default false, only replaces first occurrence)
      required:
        - path
        - old_str
        - new_str

  - name: bash
    description: Execute a bash command
    parameters:
      type: object
      properties:
        command:
          type: string
          description: The bash command to execute
      required:
        - command

  - name: search
    description: Unified search tool for finding text content or files
    parameters:
      type: object
      properties:
        query:
          type: string
          description: Text to search for or file name/path pattern
        search_type:
          type: string
          enum:
            - text
            - files
            - both
          description: "Type of search: 'text' for content search, 'files' for file names, 'both' for both (default: 'both')"
        include_pattern:
          type: string
          description: "Glob pattern for files to include (e.g. '*.ts', '*.js')"
        exclude_pattern:
          type: string
          description: "Glob pattern for files to exclude (e.g. '*.log', 'node_modules')"
        case_sensitive:
          type: boolean
          description: Whether search should be case sensitive (default false)
        whole_word:
          type: boolean
          description: Whether to match whole words only (default false)
        regex:
          type: boolean
          description: Whether query is a regex pattern (default false)
        max_results:
          type: integer
          description: Maximum number of results to return (default 50)
        file_types:
          type: array
          items:
            type: string
          description: "File types to search (e.g. ['js', 'ts', 'py'])"
        include_hidden:
          type: boolean
          description: Whether to include hidden files (default false)
      required:
        - query

  - name: create_todo_list
    description: Create a new todo list for planning and tracking tasks
    parameters:
      type: object
      properties:
        todos:
          type: array
          description: Array of todo items to create
          items:
            type: object
            properties:
              id:
                type: string
                description: Unique identifier for the todo item
              content:
                type: string
                description: Description of the todo item
              status:
                type: string
                enum:
                  - pending
                  - completed
                description: Current status of the todo item (default pending)
            required:
              - id
              - content
              - status
      required:
        - todos

  - name: update_todo_list
    description: Update existing todos in the todo list by ID
    parameters:
      type: object
      properties:
        updates:
          type: array
          description: Array of todo updates
          items:
            type: object
            properties:
              id:
                type: string
                description: ID of the todo item to update
              status:
                type: string
                enum:
                  - pending
                  - completed
                description: New status for the todo item
              content:
                type: string
                description: New content for the todo item
              priority:
                type: string
                enum:
                  - high
                  - medium
                  - low
                description: New priority for the todo item
            required:
              - id
      required:
        - updates

  - name: database_query
    description: |
      Execute READ-ONLY SQL queries (SELECT, SHOW, DESCRIBE) for data exploration and analysis.
      Returns results immediately without confirmation. For data transformations that write/modify
      data (INSERT, UPDATE, DELETE, CREATE TABLE, etc.), use ml_data instead.

      DATABASE ENGINE: DuckDB
      - Use DuckDB-compatible SQL syntax
      - For metadata: Use SHOW TABLES, SHOW VIEWS, or information_schema
      - DO NOT use SQLite-specific syntax like sqlite_master
    parameters:
      type: object
      properties:
        query:
          type: string
          description: SQL query to execute
        target_db:
          type: string
          enum:
            - system
            - user
          description: |
            Target database (default: 'user'):
            - 'system': Arc ML metadata (models, jobs, training_runs, evaluation_runs, ml_plans, data_processors) - read-only
            - 'user': User's training/application data (full SQL access)

            IMPORTANT: Always specify target_db='system' when querying ML metadata tables like:
            - jobs (training jobs, evaluation jobs)
            - models (registered models)
            - training_runs (training metrics and logs)
            - evaluation_runs (evaluation results)
            - ml_plans (ML workflow plans)
            - data_processors (data processing pipelines)
        validate_schema:
          type: boolean
          description: Whether to validate query against database schema (default true)
      required:
        - query

  - name: schema_discovery
    description: Discover and explore database schema information
    parameters:
      type: object
      properties:
        action:
          type: string
          enum:
            - list_tables
            - describe_table
            - show_schema
          description: "Schema discovery action: 'list_tables' shows all tables, 'describe_table' shows detailed table structure, 'show_schema' shows complete database overview"
        target_db:
          type: string
          enum:
            - system
            - user
          description: "Target database: 'system' for Arc metadata, 'user' for training data (default: system)"
        table_name:
          type: string
          description: Specific table name (required for 'describe_table' action)
      required:
        - action

  - name: ml_plan
    description: |
      Create or revise comprehensive ML workflow plan for a modeling task. This tool
      analyzes data, user requirements, and creates a detailed plan covering feature
      engineering, model architecture, and training/validation strategy (including
      validation metrics to track during training).

      IMPORTANT: Use this tool AFTER exploring the data with schema_discovery and database_query.
      Include data insights in the instruction parameter for better plan quality.

      Use for:
      - Initial plan creation: Provide name, instruction with ML problem description AND data insights
        from exploration (e.g., "Build classifier for X. Data insights: 1000 samples, binary
        classification (70/30 split), 5 numeric features with range 0-100, no missing values.")
      - Plan revision: Use existing plan's name and provide instruction describing the requested changes
        (e.g., "change the model architecture to use 4 layers", "use different features")

      The name parameter is used for versioning (e.g., 'diabetes-classifier' becomes 'diabetes-classifier-v1').
      The instruction parameter is used for both initial creation and revisions.

      Returns JSON with the following structure:
      {
        "status": "accepted" | "cancelled",
        "plan_id": "plan-id-v1",
        "plan": {
          "data_plan": "Feature engineering strategy...",
          "model_plan": "Architecture and training approach...",
          "knowledge": {"data": ["knowledge_id1"], "model": ["knowledge_id2"]}
        }
      }

      The plan content is always included regardless of status, allowing you to discuss it with
      the user. If status is "accepted", proceed with ml_data and ml_model using the plan_id.
      If status is "cancelled", ask the user what they would like to change.
    parameters:
      type: object
      properties:
        name:
          type: string
          description: |
            Name for the ML plan, used for versioning and identification (e.g., 'diabetes-classifier',
            'fraud-detection-model'). This name will be used to track plan versions in the database.
            Choose a descriptive, kebab-case name suitable for versioning.
        instruction:
          type: string
          description: |
            For initial creation: Description of the ML problem, goals, requirements, AND
            data insights from exploration (sample count, class distribution, feature ranges,
            missing values, correlations, etc.). Including data insights leads to better plans.

            For revisions: Description of the changes to make to the existing plan
            (e.g., "change the model architecture to use 4 layers", "use different features",
            "adjust training strategy to use 100 epochs").

            Example initial instruction with insights:
            "Build binary classifier for diabetes prediction. Data insights: 768 samples,
            binary outcome (500 positive, 268 negative = 35% positive rate), 8 numeric
            features (glucose 0-200, BMI 0-67, age 21-81), no missing values."
        source_tables:
          type: string
          description: Comma-separated list of database tables to analyze (e.g., 'users,transactions' or 'pidd')
      required:
        - name
        - instruction
        - source_tables

  - name: ml_model
    description: |
      Generate unified model + training specification and launch training. This tool:
      1) Generates model architecture AND training config in a single unified YAML
      2) Presents interactive confirmation workflow
      3) Auto-registers model to database after approval
      4) Creates trainer from training config
      5) Launches training immediately

      This tool replaces the old separate ml_model + ml_train workflow. If plan_id is provided,
      uses unified model_plan guidance (architecture + training).

      IMPORTANT: When plan_id is provided, data_processing_id is REQUIRED. Extract data_processing_id
      from ml_data's JSON result and pass it to this tool. This links the model to the specific
      data processing execution. The tool will ERROR if plan_id is provided without data_processing_id.

      Returns JSON with the following structure:
      {
        "status": "accepted" | "cancelled",
        "model_id": "model-v1",
        "model_spec": {
          "inputs": {...},
          "graph": {...},
          "outputs": {...},
          "loss": {...},
          "training": {...}
        },
        "training": {
          "status": "submitted" | "failed" | "not_started",
          "job_id": "job_xyz789",
          "train_table": "training_data",
          "error": "..." // only if failed
        }
      }

      The model_spec contains the full architecture and training configuration. Extract job_id
      to monitor training progress with /ml jobs status.
    parameters:
      type: object
      properties:
        name:
          type: string
          description: Experiment name (used for both model and trainer)
        instruction:
          type: string
          description: Description of model architecture and training requirements
        data_table:
          type: string
          description: Database table to profile for generation
        train_table:
          type: string
          description: Training data table (defaults to data_table if not provided)
        target_column:
          type: string
          description: Target column for prediction
        plan_id:
          type: string
          description: |
            Optional ML plan ID (e.g., 'pidd-plan-v1'). When provided, loads plan
            from database and uses its unified model_plan guidance.
        data_processing_id:
          type: string
          description: |
            Optional data processing execution ID from ml_data tool result. When provided,
            loads SQL transformations and output schemas to understand feature engineering.
            Extract this from ml_data's result metadata: result.metadata.data_processing_id
      required:
        - name
        - instruction
        - data_table
        - target_column

  - name: ml_evaluate
    description: |
      Evaluate a trained model on a test dataset and compute metrics.
      This is an AD HOC tool for evaluating trained models on separate test datasets.
      It is NOT part of the core ML workflow (plan → data → model → train). Validation
      metrics are already tracked during training.

      This tool:
      1) Infers target column from the model's specification
      2) Creates evaluator configuration automatically
      3) Auto-registers evaluator to database
      4) Launches evaluation job and returns immediately with job_id

      Evaluation runs asynchronously in the background. Monitor progress via /ml jobs status
      and view metrics in TensorBoard. Supports classification metrics (accuracy, precision,
      recall, F1, confusion matrix, PR curves, ROC curves) and regression metrics (MSE, MAE,
      R², scatter plots).

      Use this tool when you need to:
      - Evaluate a trained model on a hold-out test set
      - Compare multiple models on the same test data
      - Generate comprehensive evaluation reports with visualizations

      Returns JSON with the following structure:
      {
        "status": "accepted" | "cancelled",
        "evaluator_id": "evaluator-id-v1",
        "evaluation": {
          "status": "submitted" | "failed" | "not_started",
          "job_id": "job_abc123",
          "model_id": "model-v1",
          "dataset": "test_data",
          "error": "..." // only if failed
        }
      }

      The evaluation runs asynchronously. Extract job_id to monitor progress with /ml jobs status.
      View detailed metrics and visualizations in TensorBoard.
    parameters:
      type: object
      properties:
        model_id:
          type: string
          description: Model ID with version (e.g., 'my-model-v1')
        data_table:
          type: string
          description: Test dataset table name for evaluation
        metrics:
          type: array
          items:
            type: string
          description: |
            Optional list of specific metrics to compute. If not provided, metrics are
            inferred from the model's task type (classification or regression).
            Available classification metrics: accuracy, precision, recall, f1_score, auc
            Available regression metrics: mse, mae, rmse, r2_score
        output_table:
          type: string
          description: |
            Optional table name to save predictions with features and targets.
            If not provided, predictions are not saved to database.
      required:
        - model_id
        - data_table

  - name: ml_data
    description: |
      Generate and execute SQL data processing pipelines for WRITE OPERATIONS.
      Use when users describe data transformations, feature engineering, or operations
      that create/modify tables (INSERT, UPDATE, DELETE, CREATE TABLE, etc.).
      Workflow:
      1) Generate YAML from description,
      2) Show user for review/editing,
      3) Execute pipeline after confirmation,
      4) Create output tables.
      For read-only SELECT queries, use database_query for immediate results.
      If plan_id is provided, the agent loads the plan from database and uses its
      feature engineering guidance.

      Returns JSON with the following structure:
      {
        "status": "accepted" | "cancelled",
        "data_processing_id": "data_abc123",
        "execution": {
          "output_tables": ["table1", "table2"],
          "sql_operations": ["CREATE TABLE table1 AS SELECT...", "CREATE TABLE table2 AS SELECT..."]
        }
      }

      The data_processing_id is REQUIRED for ml_model when using a plan_id. Extract it from
      the JSON and pass to ml_model. The sql_operations provide context about the DB transactions
      performed.
    parameters:
      type: object
      properties:
        name:
          type: string
          description: Name for the data processor (will be registered in database)
        instruction:
          type: string
          description: Natural language description of data processing requirements
        source_tables:
          type: array
          items:
            type: string
          description: List of source database tables to analyze for generation (required to narrow scope)
        database:
          type: string
          enum:
            - system
            - user
          description: "Target database: 'system' for Arc metadata, 'user' for training data (default: user)"
        plan_id:
          type: string
          description: |
            Optional ML plan ID to use for data processing (e.g., 'pidd-plan-v1').
            When provided, the agent loads the plan from database and uses its
            feature engineering guidance (feature_engineering section).
        recommended_knowledge_ids:
          type: array
          items:
            type: string
          description: |
            Optional list of knowledge IDs recommended by ml_plan (e.g., ['mlp', 'dcn']).
            When provided from ml_plan result, these are displayed in the section metadata
            for visibility and can inform data processing decisions.
      required:
        - name
        - instruction
        - source_tables

  - name: read_knowledge
    description: |
      Read domain knowledge documents to inform model/trainer/evaluator generation.
      Knowledge includes architectures (dcn, transformer), patterns (feature-interaction,
      regularization), and scenarios (fraud-detection, churn-prediction).

      Use this tool to:
      - Understand architecture details and implementation guidance
      - Learn problem-specific patterns and best practices
      - Discover relevant techniques and patterns

      Available knowledge is listed in your context. Read relevant documents before
      generating specifications to ensure high-quality outputs.
    parameters:
      type: object
      properties:
        knowledge_id:
          type: string
          description: "ID of the knowledge to read (e.g., 'dcn', 'feature-interaction')"
        phase:
          type: string
          enum: [model, trainer, evaluator]
          description: "Which phase guide to read (default: model)"
      required:
        - knowledge_id
